---
title: "Project 1"
author: "Jennifer Nguyen (JNN497)"
date: "3/12/2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```


##**Introduction**

I chose the topic of movies for my project because I'm an avid movie watcher. I tend to go see most if not all blockbuster hits in theatres as soon as they're released. If I'm not at the movies, I also have the habit of rewatching films I like over and over at home. 

The dataset 'imdb' contains information from the IMDb website about a certain selection of movies released between the years 2006-2016.  The variables in the 'imdb' dataset include the rank, title, genre, general description, year, director, actors, runtime (in minutes), rating, votes that contributed to that rating, revenue (in millions), and a metascore rating. This dataset was found online at kaggle.com. The dataset 'fandango' contains information about a different selection of films written for an article for the newsletter fivethirtyeight. The variables in this dataset include ratings from Rotten Tomatoes, IMDb, Metacritic, and the respective normalized and rounded scores for each rating. Other variables also included vote counts for each rating, years, and the title of each film. This dataset was found on the preloaded packages called 'fivethirtyeight'. The expectations I think I will see are that the ranks, ratings, and metascore ratings from the 'imdb' dataset will most likely by in correlation with the Rotten Tomatoes, IMDb, and Metacritic ratings from the 'fandango' dataset for any films that are in both datasets. My reasoning behind this expectation is that most movie ratings are done from professional critics who generally have the same consensus when rating movies. 

##**Joining**
``` {R}
install.packages("tidyverse")
install.packages("readr", dependencies=TRUE, INSTALL_opts = c('--no-lock'))
library(tidyverse)
imdb <- read.csv(file = "IMDB-Movie-Data.csv")
glimpse(imdb)
library(fivethirtyeight)
fandango <- as.data.frame(fandango)
glimpse(fandango)
fandango <- fandango %>% rename(Title=film) %>% select(-year)
films <- imdb %>% inner_join(fandango)
glimpse(films)
imdb %>% anti_join(fandango) %>% summarize(n())
``` 

The first step was to import both datasets; the 'imdb' dataset was found online at Kaggle.com and the 'fandango' dataset was found as a package on R. I also dropped the 'year' variable from the original 'fandango' dataset since the 'imdb' dataset had its own 'Year' variable with differing observations. In the 'fandango' dataset, the 'title' variable was renamed to 'Title' variable in order to join it together with the corresponding 'Title' variable in the 'imdb' dataset. Then, I used the command 'inner_join' in order create a new 'films' dataset, while dropping any rows that didn't have a match in 'imdb' and 'fandango' datasets. By doing this, I would be left with only the movies that had their respective rating and descriptive information in both datasets. This would ensure that my future summary statistics would have no NA's in either dataset. In other words, that means that the cases that were dropped would be movies that had no information in the other dataset. For example, the movie '12 Years A Slave' from the 'imdb' dataset was not seen in the 'fandango' dataset, indicating that it was most likely not used in fivethirtyeight's newsarticle about fandango ratings. I also used the commands 'anti_join', 'count', and 'summarize' to count 941 rows that were dropped after joining the two datasets. The potential problem of dropping so many rows would be that my dataset would be representative of a very miniscule portion of the movie industry, instead of the industry as a whole. There is also a chance of having an unequal amount of movies for each year for the 'films' dataset. 

   

##**Wrangling and Tidying**
```{R}
#summary statistic 1
films %>% filter(Year == "2015")%>% 
  mutate(Runtime..Hrs = Runtime..Minutes./60) %>%
  select(Title, Runtime..Minutes., Runtime..Hrs) %>%
  arrange(desc(Runtime..Hrs)) %>% glimpse()
#summary statistic 2
films2 <- films %>% separate(Genre,into = c("genre1","genre2", "genre3"), sep= ",") 
films2 %>% group_by(genre1, Year) %>% summarize("count" = n()) %>%
  pivot_wider(names_from = "genre1", values_from = "count")
#summary statistic 3-6
films %>% summarize(min(rottentomatoes), max(rottentomatoes), median(rottentomatoes),
                    IQR(rottentomatoes))
#summary statistic 7-9
films %>% group_by(Year) %>% summarize(mean(Runtime..Minutes.), sd(Runtime..Minutes.),
                                       var(Runtime..Minutes.))
#summary statistic 10-11
films2 %>% group_by(genre1) %>% summarize(first(Metascore), last(Metascore))
#summary statistic 12
films %>% select(rottentomatoes, metacritic, imdb) %>% cor

```

Statistic 1 showcases the runtime in both hours and minutes of movies only released in 2015, and utilizes the dplyr functions 'filter', 'arrange', 'select' and 'mutate'. I mutated the runtime in minutes in order to generate the runtime in hours. Statistic 2 showcases the number of movies categorized by genre that were released in each year, and utilizes the last 2 core dplyr functions 'group_by' and 'summarize'. Since the original 'genre' variable had multiple genres within it, I ended up separating the variable in order to create a general genre variable called 'genre1'. The other genres that did not fall into the 'genre1' variable were put under 'genre2' and 'genre3' variables. I saved this dataset with the separated genre variables as a new 'film2' dataset in order to use it again later on in this project. Statistic 2 also grouped by 2 categorical variables, 'genre1' and 'year'. I also illustrated **tidying** with the 'pivot_wider' command in statistic 2, giving each genre their own variable in order to better organize the counts of each movie of each year. This statistic also illustrates grouping by two categorical variables.


Statistic 3-6 focuses on Rotten Tomato ratings, by summarizing 4 statistics; the minimum, maximum, median, and IQR of the Rotten Tomato ratings in the dataset. Statistic 7-9 summarizes 3 more statistics; the mean, standard deviation, and variation of the runtime (in minutes) of each (or grouped by) the categorical variable 'year'. Statistic 10-11 summarizes 2 more additional statistics by summarizing the first and last Metascore rating of each general genre listed by rank. Lastly, statistic 12, showcases a correlation of the numeric variables of Rotten Tomatoes ratings, Metacritic ratings, and IMDb ratings.

##**Visualizing**
``` {R}
films3 <- films %>% select(Rating, Runtime..Minutes., rottentomatoes, metacritic, imdb)
films3 %>% select_if(is.numeric) %>% cor %>%
  as.data.frame %>% rownames_to_column %>% 
  pivot_longer(-1) %>% ggplot(aes(rowname,name,fill=value)) +
  geom_tile() + geom_text(aes(label=round(value,2)))+
  xlab("")+ylab("") + theme(axis.text.x = element_text(angle=45, hjust=1)) + 
  scale_fill_gradient2(low="yellow",high="red") + theme_minimal() + 
  ggtitle("Correlation heatmap of films3 dataset")
```
Please note that the new 'films3' dataset was generated with five numeric variables. The original 'films' dataset had 26 numeric variables and thus would have generated a confusing and garbled correlation heatmap. 



``` {R}
films2 %>%  ggplot(aes(x = Rank, y = Revenue..Millions., color = genre1)) +
  geom_point() + theme_minimal() + scale_y_continuous(breaks=seq(0, 650, 100)) + 
  scale_color_brewer(palette= "Set2") +
  ggtitle("Plot of movie revenue (in millions of dollars) and ranks by genre") +
  ylab("Revenue (in millions of dollars)") 
```

This scatterplot maps 3 variables; rank, revenue (in millions), and genre. In other words, the scatterplot showcases the rank of movies and their corresponding revenue (in millions of dollars), color coded by their general genre. From the plot, it can be seen that there is a negative trend; action movies decrease in revenue as their rank increases. Overall, the same negative trend can be seen be for all the movies in the dataset, regardless of their genre. Even though the observations are assorted by genre, it should be noted that some genres have only one or a few movies so this plot may not be representative of the movie industry. 



``` {R}
films2 %>% ggplot(aes(Year, fill=genre1)) +
  geom_bar(aes(y=Revenue..Millions., position= 'fill'), stat="summary", fun.y="mean") +
  scale_fill_brewer(palette= "Pastel1") + theme_light() + 
  scale_x_continuous(breaks = c(2014, 2015)) +
  ggtitle("Bar graph of movie revenue (in millions of dollars) of each genre by year") +
  ylab("Revenue (in millions of dollars)")
```

This bar graph maps 3 variables; year, revenue (in millions), and genre. In other words, the bar graph showcases how much revenue was made each year, and the proportions of how much each genre made in that year. It can be seen that the adventure genre made the most revenue in 2014. However, in 2015, the animation genre made the most revenue. For both years, it can be seen that crime, mystery, and horror genres made only a small portion of the revenue in their respective years. Overall, the 2014 movies made less revenue when compared to the 2015 movies in this dataset.


##**Dimensionality Reduction**
```{R}
films4 <- films %>% select(Year, Rank, Rating, rottentomatoes, 
                           Metascore, imdb, Revenue..Millions., fandango_stars)

library(cluster)
pam_dat <- films4 %>% select(-Year) %>% scale()
sil_width<-vector() 
for(i in 2:10){
  pam_fit <- pam(pam_dat, k = i)
  sil_width[i] <- pam_fit$silinfo$avg.width 
  } 
ggplot() + geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)
pam <- films4 %>% select(-Year) %>% scale() %>% pam(k=2)
pam
install.packages("GGally")
library(GGally)
plot(pam, which=2)
films4 %>% mutate(cluster=as.factor(pam$clustering)) %>% ggpairs(columns = 2:8, aes(color=cluster))
```


For the clustering analysis, the 'films3' dataset was generated with 8 numerical variables; the original dataset had 26 numerical variables and would have generated  confusing plots and visualizations. The first step in the cluster analysis was to process the data and scale it. For the second step, the first plot was generated so I could look at my mean silhouette width in order to choose the number of clusters to use for PAM analysis. Since the "elbow" peaks at 2 and gradually decreases, this indicates that k = 2 or that 2 clusters will be used in this analysis. For the third through fifth steps, the PAM analysis was ran, the clusters were interpreted, and then visualized. 

Pertaining to the silhouette plot or the second plot, it can be said that the cluster structure is weak and possibly artificial since the average silhouette widths were between 0.26-0.50. The third plot visualizes all pairwise combinations of the 7 variables. It can be said that  most of the variables do not map nicely since most of the clusters can be seen to be overlapping. The clusters can be seen most distinctly when comparing the 'rottentomatoes' and 'Rating' variables plot or the 'Metascore' and 'Rating' variables plot. 
